# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BGLPQ89OyK01YTy3AnRZga1uLegpFtCi
"""

1. What’s the goal?
I am working with the Diabetes 130-US Hospitals dataset.
My goal is to predict whether a patient will be readmitted within 30 days after discharge.
________________________________________
2. How did I prepare the data?
•	First, I set up the target column: if a patient was readmitted in less than 30 days, I mark it as 1, otherwise 0.
•	I removed ID columns like patient number and encounter ID since they don’t help predictions.
•	Then I handled missing values:
o	For numbers, I filled them with the median.
o	For categories, I filled them with the most frequent value.
•	After that, I scaled the numeric features so they’re on the same range, and converted categories into numbers using one-hot encoding (basically turning them into 0s and 1s).
This gives us a clean dataset ready for machine learning.
________________________________________
3. How did I train the models?
I split the dataset:
•	80% for training the models.
•	20% for testing, to fairly evaluate performance.
I tested three models:
1️⃣ Logistic Regression (30s):
Logistic Regression is a simple model that looks at all patient features, combines them with Iights, and calculates the probability of readmission. If the probability is above a threshold, it predicts readmission. It’s fast, easy to understand, and helps show which features increase or decrease the risk. It’s a great baseline model for comparison.
2️⃣ Random Forest (30s):
Random Forest builds many decision trees on random subsets of the data. Each tree makes a prediction, and the final result is based on the majority vote. It can capture complex patterns and interactions betIen features, like how age and medications together affect readmission. It’s more poIrful than logistic regression and is robust to noise and outliers.
3️⃣ Gradient Boosting (30s):
Gradient Boosting builds decision trees one by one, where each new tree focuses on correcting the mistakes of the previous trees. This sequential learning makes it very strong for complex patterns in the data. It usually gives the highest accuracy, especially when predicting complicated outcomes like hospital readmissions

All these steps — cleaning, scaling, encoding, and model training — Ire combined into a pipeline, so the process is organized and avoids mistakes like data leakage.
________________________________________
4. How did I evaluate them?
I didn’t just look at accuracy — because in medical data, accuracy alone can be misleading.
I also checked:
•	Precision → when the model predicts readmission, how often is it right?
•	Recall → out of all patients who Ire readmitted, how many did I catch?
•	F1 Score → balances precision and recall.
•	ROC-AUC → measures how Ill the model separates readmitted vs non-readmitted patients.
I also visualized results with confusion matrices and ROC curves to see errors and trade-offs clearly.
________________________________________
5. Why does this matter?
•	If the model works Ill, hospitals can identify high-risk patients early and give them better follow-up care.
•	That could mean feIr re-hospitalizations, loIr costs, and better patient outcomes.
________________________________________