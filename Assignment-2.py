# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a-XJLd4zPeR0jmMx7WGYraM74eGd_Bna
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Step 1: Load dataset (Setosa vs Versicolor only)
iris = load_iris()
X = iris.data[:100, :2]   # 2 features for visualization
y = iris.target[:100]     # 0 = Setosa, 1 = Versicolor

# Normalize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Step 2: Initialize parameters
np.random.seed(42)
weights = np.random.randn(X_train.shape[1]) * 0.1
bias = 0.0
lr = 0.1
epochs = 10

# Sigmoid activation
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Binary cross-entropy loss
def binary_cross_entropy(y_true, y_pred):
    eps = 1e-10  # avoid log(0)
    return -np.mean(y_true * np.log(y_pred + eps) + (1 - y_true) * np.log(1 - y_pred + eps))

# Step 3: Train with gradient descent
losses = []
for epoch in range(epochs):
    epoch_loss = 0
    for xi, target in zip(X_train, y_train):
        # Forward pass
        z = np.dot(xi, weights) + bias
        y_pred = sigmoid(z)

        # Loss for this sample
        loss = binary_cross_entropy(target, y_pred)
        epoch_loss += loss

        # Gradients
        error = y_pred - target
        grad_w = error * xi
        grad_b = error

        # Update parameters
        weights -= lr * grad_w
        bias -= lr * grad_b

    # Average loss over training samples
    avg_loss = epoch_loss / len(X_train)
    losses.append(avg_loss)
    print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

# Step 4: Plot loss curve
plt.plot(range(1, epochs+1), losses, marker='o')
plt.xlabel("Epochs")
plt.ylabel("Binary Cross-Entropy Loss")
plt.title("Loss vs Epochs (Perceptron with Sigmoid on Iris dataset)")
plt.grid(True)
plt.show()

# Step 5: Evaluate on test set
y_probs = sigmoid(np.dot(X_test, weights) + bias)
y_preds = (y_probs >= 0.5).astype(int)
accuracy = np.mean(y_preds == y_test)
print(f"\nTest Accuracy: {accuracy*100:.2f}%")

1. Dataset

I used the Iris dataset, which is a classic dataset in machine learning with 150 flower samples, each described by four numerical features. For simplicity, I restricted it to the first 100 samples, which correspond to two classes: Setosa and Versicolor.

I also used only the first two features â€” sepal length and sepal width â€” so that the model remains interpretable. The dataset was normalized using standard scaling to ensure all features are on the same scale.

2. Model

The model is inspired by the perceptron, but instead of using the hard step function, I used a sigmoid activation.

The sigmoid maps any real input into a range between 0 and 1, which we can interpret as a probability.

This makes the learning smoother and allows us to use binary cross-entropy loss as a measure of how well the model is performing.

The decision function is simply:

ð‘¦=Ïƒ(wâ‹…x+b)

where
ð‘¤
w are the weights,
ð‘
b is the bias, and
ðœŽ
Ïƒ is the sigmoid function.

3. Training Process

I trained the model using gradient descent.

For each training sample, the forward pass computes the predicted probability.

Then we calculate the binary cross-entropy loss. Unlike the perceptronâ€™s squared error, this loss is designed for probabilistic binary classification.

Using calculus, we compute gradients of the loss with respect to the weights and bias.

Finally, we update the parameters using the learning rate.

This loop is repeated for 10 epochs. By recording the average loss each epoch, we can track how well the model is learning.

4. Results

When I plotted loss vs epochs, I observed a smooth, gradual decline in the loss values, rather than an abrupt drop to zero like in the original perceptron. This better reflects how machine learning models typically behave.

After training, I evaluated the model on the test set and achieved high accuracy â€” showing that even with just two features, the perceptron with sigmoid activation can separate Setosa from Versicolor.

5. Key Takeaways

The classic perceptron converges instantly on linearly separable data, but by introducing sigmoid activation and cross-entropy loss, we get a more realistic training process.

This model is essentially a logistic regression, which is the probabilistic cousin of the perceptron.

Implementing it from scratch in NumPy gives a clear understanding of how gradient descent and loss minimization work behind the scenes.

import numpy as np                     # NumPy: numerical operations and arrays
import matplotlib.pyplot as plt        # Matplotlib: for plotting the loss curve
from sklearn.datasets import load_iris # sklearn helper to load Iris dataset
from sklearn.model_selection import train_test_split  # to split data into train/test
from sklearn.preprocessing import StandardScaler      # to standardize features

# -----------------------------
# 1) Load and prepare the dataset
# -----------------------------
iris = load_iris()                      # load the Iris dataset into a Bunch object
X = iris.data[:100, :2]                 # take first 100 samples (Setosa + Versicolor),
                                        # and only first 2 features (sepal length, sepal width)
y = iris.target[:100]                   # corresponding labels: 0 (Setosa) or 1 (Versicolor)

# Standardize features to zero mean and unit variance â€” helps gradient descent converge
scaler = StandardScaler()               # create a StandardScaler instance
X = scaler.fit_transform(X)             # fit scaler on X and transform X (returns numpy array)

# Split into train and test sets (80% train, 20% test), shuffle deterministic with random_state
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# -----------------------------
# 2) Initialize model & hyperparams
# -----------------------------
np.random.seed(42)                      # seed RNG for reproducible random weights
weights = np.random.randn(X_train.shape[1]) * 0.1
                                        # initialize weights (one per feature).
                                        # small random values -> avoid huge initial activations
bias = 0.0                              # initialize bias (scalar) to zero
lr = 0.1                                # learning rate (step size for gradient descent)
epochs = 10                             # total number of training epochs (as you requested)

# -----------------------------
# 3) Helper functions
# -----------------------------
def sigmoid(z):
    """Sigmoid activation: maps real-valued input to (0,1)."""
    return 1.0 / (1.0 + np.exp(-z))     # elementwise for numpy arrays

def binary_cross_entropy(y_true, y_pred):
    """
    Binary cross-entropy loss for a single sample or array.
    y_true: 0 or 1 (or array of those)
    y_pred: predicted probability in (0,1) (or array)
    We add eps to avoid log(0).
    """
    eps = 1e-10                         # tiny value to keep log safe
    # formula: -[ y*log(p) + (1-y)*log(1-p) ], averaged if arrays are passed
    return -np.mean(y_true * np.log(y_pred + eps) + (1 - y_true) * np.log(1 - y_pred + eps))

# -----------------------------
# 4) Training loop (stochastic-like: sample-by-sample updates)
# -----------------------------
losses = []                             # store average loss per epoch for plotting
for epoch in range(epochs):             # loop over epochs
    epoch_loss = 0.0                    # accumulator for total loss this epoch

    # loop over each training sample (xi is shape (n_features,), target is scalar 0/1)
    for xi, target in zip(X_train, y_train):
        z = np.dot(xi, weights) + bias # linear combination: w^T x + b -> scalar
        y_pred = sigmoid(z)            # pass through sigmoid -> predicted probability in (0,1)

        # compute loss for this sample (scalar)
        loss = binary_cross_entropy(target, y_pred)
        epoch_loss += loss             # accumulate loss (we'll average later)

        # compute gradients for logistic loss (derivation: dL/dz = y_pred - y_true)
        error = y_pred - target        # scalar gradient of loss w.r.t. linear output z
        grad_w = error * xi            # gradient w.r.t weights: (y_pred - y) * x_i  -> vector shape (n_features,)
        grad_b = error                 # gradient w.r.t bias is just (y_pred - y)

        # parameter update (gradient descent step)
        # we subtract lr * gradient because we want to move in direction that decreases loss
        weights -= lr * grad_w         # update weight vector in-place
        bias -= lr * grad_b            # update bias in-place

    # after iterating all samples, compute average loss for the epoch
    avg_loss = epoch_loss / len(X_train)
    losses.append(avg_loss)            # store for plotting
    # print progress so you can watch the loss drop epoch by epoch
    print(f"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.6f}")

# -----------------------------
# 5) Plot loss vs epochs
# -----------------------------
plt.plot(range(1, epochs+1), losses, marker='o')   # x: epochs (1..epochs), y: avg loss per epoch
plt.xlabel("Epochs")
plt.ylabel("Binary Cross-Entropy Loss")
plt.title("Loss vs Epochs (Sigmoid Perceptron / Logistic model)")
plt.grid(True)
plt.show()

# -----------------------------
# 6) Evaluate on test set
# -----------------------------
# compute predicted probabilities for all test samples in one vectorized operation
y_probs = sigmoid(np.dot(X_test, weights) + bias)  # shape: (n_test_samples,)
y_preds = (y_probs >= 0.5).astype(int)             # threshold at 0.5 to get class labels 0/1
accuracy = np.mean(y_preds == y_test)              # fraction of correct predictions
print(f"\nTest Accuracy: {accuracy*100:.2f}%")

# Optional: print a couple of predictions with their probabilities for inspection
for i in range(min(5, len(X_test))):
    print(f"X_test[{i}] -> prob={y_probs[i]:.3f}, pred={y_preds[i]}, true={y_test[i]}")