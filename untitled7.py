# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gD5aEFPxT2DVazTeT83oNKkryrfV6qh3
"""

# Diabetes readmission prediction
# Dataset: Diabetes 130-US hospitals for years 1999â€“2008
# File assumed: diabetic_data.csv in the same folder

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    confusion_matrix, ConfusionMatrixDisplay, roc_curve
)

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

# -----------------------------
# 1. Load the dataset
# -----------------------------
df = pd.read_csv("diabetic_data.csv")
print("Dataset shape:", df.shape)

print(df.head())





# -----------------------------
# 2. Target variable
# -----------------------------
# We use 'readmitted':
# '<30' => readmitted within 30 days -> label 1
# 'NO' or '>30' => not readmitted within 30 days -> label 0
df["target"] = df["readmitted"].apply(lambda x: 1 if x == "<30" else 0)

print(df["target"].value_counts(normalize=True))  # check balance of target

# -----------------------------
# 3. Drop identifier columns
# -----------------------------
df.drop(["encounter_id", "patient_nbr"], axis=1, inplace=True, errors="ignore")

# -----------------------------
# 4. Separate features and target
# -----------------------------
X = df.drop(columns=["target", "readmitted"])  # features
y = df["target"]                                # labels

# -----------------------------
# 5. Identify categorical vs numeric features
# -----------------------------
cat_cols = X.select_dtypes(include=["object"]).columns.tolist()
num_cols = X.select_dtypes(exclude=["object"]).columns.tolist()

print("Categorical columns:", len(cat_cols))
print("Numerical columns:", len(num_cols))

# -----------------------------
# 6. Preprocessing pipeline
# -----------------------------
# - Numeric: impute missing values, then normalize (StandardScaler)
# - Categorical: impute missing values, then one-hot encode
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, num_cols),
        ("cat", categorical_transformer, cat_cols)
    ]
)

# -----------------------------
# 7. Train-test split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

print("Train size:", X_train.shape, "Test size:", X_test.shape)

# -----------------------------
# 8. Define classifiers
# -----------------------------
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight="balanced"),
    "Random Forest": RandomForestClassifier(n_estimators=200, class_weight="balanced", random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=200, random_state=42)
}

# -----------------------------
# 9. Train, predict, evaluate
# -----------------------------
results = {}

for name, model in models.items():
    # Combine preprocessing + model into one pipeline
    clf = Pipeline(steps=[("preprocessor", preprocessor),
                         ("classifier", model)])

    # Fit model
    clf.fit(X_train, y_train)

    # Predictions
    y_pred = clf.predict(X_test)
    y_prob = clf.predict_proba(X_test)[:, 1] if hasattr(clf, "predict_proba") else None

    # Metrics
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob) if y_prob is not None else np.nan

    results[name] = [acc, prec, rec, f1, auc]

    # Print report
    print(f"\n{name} Results:")
    print(f"Accuracy:  {acc:.3f}")
    print(f"Precision: {prec:.3f}")
    print(f"Recall:    {rec:.3f}")
    print(f"F1-score:  {f1:.3f}")
    print(f"ROC AUC:   {auc:.3f}")

    # Confusion matrix plot
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap="Blues")
    plt.title(f"{name} - Confusion Matrix")
    plt.show()

    # ROC curve plot (if probabilities available)
    if y_prob is not None:
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        plt.plot(fpr, tpr, label=f"{name} (AUC={auc:.2f})")
        plt.plot([0,1], [0,1], "k--")
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.title("ROC Curve")
        plt.legend()
        plt.show()

# -----------------------------
# 10. Compare results
# -----------------------------
results_df = pd.DataFrame(results, index=["Accuracy", "Precision", "Recall", "F1", "ROC AUC"])
print("\nComparison of Models:")
print(results_df)